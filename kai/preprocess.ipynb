{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =np.load(\"../data/train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-490dc6026a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_para/word_freq\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "min_c=3\n",
    "paragraph = set(train[:,2])\n",
    "\n",
    "all_context = []\n",
    "word_freq=defaultdict(int)\n",
    "front = \"1\"\n",
    "for para in paragraph:\n",
    "    p=[]\n",
    "    #add start and end\n",
    "    #p.append(\"<start>\")\n",
    "    for letter in para:\n",
    "        p.append(letter)\n",
    "        word_freq[letter]+=1\n",
    "    #p.append(\"<end>\")\n",
    "    all_context.append(p)\n",
    "Nu = \"@@\"\n",
    "wordoffew=[]\n",
    "for i,row in enumerate(all_context):\n",
    "    for j,col in enumerate(row):\n",
    "        if word_freq[col]<min_c:\n",
    "            wordoffew.append(col)\n",
    "            all_context[i][j]=Nu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_config = [word_freq,min_c,Nu]\n",
    "\n",
    "with open(\"model_para/word_freq.pk\",\"wb\") as pk:\n",
    "    pickle.dump(w2v_config,pk)\n",
    "    pk.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(wordoffew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_c=3\n",
    "QA_word2vec=Word2Vec(all_context,iter=50,min_count=min_c,size=256,window=20,workers=16)\n",
    "\n",
    "vocab = dict([(k, v.index) for k, v in QA_word2vec.wv.vocab.items()])\n",
    "weight_matrix = QA_word2vec.wv.syn0 #word_to_vec\n",
    "\n",
    "QA_word2vec.save(\"model_para/QA_word2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('彗', 0.31442469358444214),\n",
       " ('歇', 0.29723507165908813),\n",
       " ('繆', 0.2952030599117279),\n",
       " ('托', 0.2632206678390503),\n",
       " ('吁', 0.26204097270965576),\n",
       " ('偶', 0.2530558407306671),\n",
       " ('泣', 0.25270524621009827),\n",
       " ('星', 0.24661768972873688),\n",
       " ('譜', 0.2448553442955017),\n",
       " ('韋', 0.24352028965950012)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_word2vec.most_similar(positive=[\"漂\",\"亮\",\"貝\"],negative=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_word2vec= Word2Vec.load(\"model_para/QA_word2vec.bin\")\n",
    "vocab = dict([(k, v.index) for k, v in QA_word2vec.wv.vocab.items()])\n",
    "weight_matrix = QA_word2vec.wv.syn0 #word_to_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of context:  1158\n"
     ]
    }
   ],
   "source": [
    "max_len=0\n",
    "for row in all_context:\n",
    "    if len(row)>max_len:\n",
    "        max_len = len(row)\n",
    "\n",
    "print('max length of context: ',max_len)\n",
    "\n",
    "answer_ptr = np.apply_along_axis(lambda x:(int(x[5]),int(x[5])+len(x[6])) , axis=1 , arr = train)\n",
    "answer_ptr.shape , train.shape\n",
    "np.save(\"../data/y_ptr.npy\" , answer_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 316)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "words = jieba.cut(train[i][2], cut_all=False)\n",
    "a = []\n",
    "for word in words:\n",
    "    a.append(word)\n",
    "\n",
    "len(a) , len(train[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba教學\n",
    "http://blog.fukuball.com/ru-he-shi-yong-jieba-jie-ba-zhong-wen-fen-ci-cheng-shi/\n",
    "  \n",
    "需下載 dict.txt.big 以轉成繁體中文模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/derricksu/Chinese_QA/data/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u2c8973e8f27ca5fa986cc90b0a140f46.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input： 哈囉你好嗎\n",
      "我今天想吃牛排\n",
      "而我聽見下雨的聲音\n",
      "想起妳用唇語說愛情\n",
      "今天不回家\n",
      "只是個乾妹妹\n",
      "你妹妹在哪\n",
      "Output 精確模式 Full Mode：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.938 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('../data/dict.txt.big')\n",
    "\n",
    "content = open('lyric.txt', 'r').read()\n",
    "\n",
    "print(\"Input：\", content)\n",
    "\n",
    "words = jieba.cut(content.split(\"\\n\")[0], cut_all=False)\n",
    "\n",
    "print(\"Output 精確模式 Full Mode：\")\n",
    "lyric = []\n",
    "for word in words:\n",
    "    lyric.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['哈囉', '你好', '嗎']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/derricksu/Chinese_QA/data/dict.txt.big ...\n",
      "Dumping model to file cache /tmp/jieba.u2c8973e8f27ca5fa986cc90b0a140f46.cache\n",
      "Loading model cost 1.313 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "max_len=0\n",
    "for para in train[:,2]:\n",
    "    words = jieba.cut(para, cut_all=False)\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        c+=1\n",
    "    if c>max_len:\n",
    "        max_len = c\n",
    "print(\"max length of paragraph after jieba :\",max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/dict.txt.big\")\n",
    "res.text[0:100]\n",
    "\n",
    "f = open(\"dict.txt.big\",\"w\")\n",
    "f.write(res.text)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
