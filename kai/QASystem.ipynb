{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "    P -> Encoder-lstm -> match-lstm -> PTR-Net\n",
    "    Q -> Encoder-lstm -^ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Input ,Lambda , Dropout\n",
    "from keras.layers import Dense ,TimeDistributed ,LSTM \n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import merge,Dot,Multiply,Reshape,concatenate,RepeatVector\n",
    "from keras.preprocessing import sequence,text\n",
    "\n",
    "import keras.backend as K \n",
    "#from keras.backend import softmax #無法設定axis\n",
    "from keras.activations import softmax #可設定axis\n",
    "from keras.callbacks import History,EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "#train\n",
    "train =np.load(\"../data/train.npy\")\n",
    "\n",
    "#label\n",
    "#ans start , ans end+1\n",
    "ptr_train = np.load(\"../data/y_ptr.npy\")\n",
    "\n",
    "#test\n",
    "test =np.load(\"../data/test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load word2vec\n",
    "QA_word2vec = Word2Vec.load(\"model_para/QA_word2vec.bin\")\n",
    "weight_matrix = QA_word2vec.wv.syn0\n",
    "vocab = dict([(k, v.index) for k, v in QA_word2vec.wv.vocab.items()])\n",
    "\n",
    "## word_freq , use \"rb\"\n",
    "pk = open(\"model_para/word_freq.pk\",\"rb\")\n",
    "word_freq,min_c,Nu = pickle.load(pk)\n",
    "\n",
    "pk.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of context : 1158\n",
      "max length of question : 331\n"
     ]
    }
   ],
   "source": [
    "## train paragraph process\n",
    "paragraph_idx = []\n",
    "p_max_len = 0\n",
    "for para in train[:,2]:\n",
    "    context = []\n",
    "    for letter in para:\n",
    "        if word_freq[letter]<min_c:\n",
    "            context.append( vocab[Nu] )\n",
    "        else:\n",
    "            context.append( vocab[letter] )\n",
    "    paragraph_idx.append(context)\n",
    "    if p_max_len < len(context):\n",
    "        p_max_len = len(context)\n",
    "\n",
    "print(\"max length of context :\" ,p_max_len)\n",
    "\n",
    "## question process\n",
    "question=[]\n",
    "q_max_len = 0\n",
    "for q in train[:,4]:\n",
    "    qu = []\n",
    "    for letter in q:\n",
    "        if word_freq[letter]<min_c:\n",
    "            qu.append( vocab[Nu] )\n",
    "        else:\n",
    "            qu.append( vocab[letter] )\n",
    "    question.append(qu)\n",
    "    if q_max_len < len(context):\n",
    "        q_max_len = len(context)\n",
    "\n",
    "print(\"max length of question :\" , q_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer ptr\n",
    "ans_ptr = []\n",
    "for i , ans in enumerate(ptr_train):\n",
    "    answer_set = np.zeros(shape = len(paragraph_idx[i]))\n",
    "    answer_set[ans[0]]=1\n",
    "    answer_set[ans[1]-1]=1\n",
    "    ans_ptr.append(answer_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer ptr\n",
    "ans_ptr_start = []\n",
    "ans_ptr_end = []\n",
    "for i , ans in enumerate(ptr_train):\n",
    "    st = np.zeros(shape = (len(paragraph_idx[i]) ))\n",
    "    end = np.zeros(shape = (len(paragraph_idx[i]) ))\n",
    "    st[ans[0]]=1\n",
    "    end[ans[1]-1]=1\n",
    "    ans_ptr_start.append(st)\n",
    "    ans_ptr_end.append(end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test paragraph data\n",
    "test_context=[]\n",
    "for para in test[:,2]:\n",
    "    context = []\n",
    "    for letter in para:\n",
    "        if word_freq[letter]>=min_c:\n",
    "            context.append(vocab[letter])\n",
    "        else:\n",
    "            context.append(vocab[Nu])\n",
    "    test_context.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14611, 1159, 1), (14611, 1159))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_review_length = p_max_len+1\n",
    "question_length = q_max_len+1\n",
    "\n",
    "p_train = sequence.pad_sequences(paragraph_idx,padding=\"post\", maxlen=max_review_length)\n",
    "q_train = sequence.pad_sequences(question,padding=\"post\", maxlen=question_length)\n",
    "ans_train_start = sequence.pad_sequences(ans_ptr_start,padding=\"post\", maxlen=max_review_length).reshape((-1,max_review_length,1))\n",
    "ans_train_end = sequence.pad_sequences(ans_ptr_end,padding=\"post\", maxlen=max_review_length).reshape((-1,max_review_length,1))\n",
    "\n",
    "#p_train = sequence.pad_sequences(paragraph_idx, maxlen=max_review_length)\n",
    "#q_train = sequence.pad_sequences(question, maxlen=question_length)\n",
    "\n",
    "\n",
    "ans_train_start.shape,p_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([239, 136,  89, ...,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_idx[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271, (1159, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_train_start[2].argmax(),ans_train_start[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271, (506,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_ptr_start[2].argmax(),ans_ptr_start[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['广州', '1',\n",
       "       '廣州市，簡稱穗，現有別稱五羊仙城、羊城、穗城、穗垣、仙城、花城，為中華人民共和國廣東省省會，中國超大城市及副省級城市，是繼上海、北京之後的中國第三大城市、國家中心城市、也是中國華南地區的經濟、文化、科技和教育中心及交通樞紐，是中國人民解放軍南部戰區聯合作戰指揮部所在地。廣州擁有2200年以上歷史，為中國首批歷史文化名城之一，是中國南方最大、歷史最悠久的對外通商口岸，也是全國首批對外開放的14個沿海城市之一，世界著名的港口城市之一。廣州港是中國第五大港口，世界第八大港口。廣州在2010年成功舉辦第16屆亞洲運動會。\\n廣州地處華南，廣東省的東南部，珠江三角洲中北緣，西江、北江、東江三江匯合處，瀕臨南中國海，東連東莞市和惠州市博羅、龍門兩縣，西鄰佛山市的三水、南海和順德三區，北靠清遠市的市區和佛岡縣及韶關市的新豐縣，南接中山市，毗鄰香港、澳門特別行政區，地理位置優越，是「海上絲綢之路」的起點之一，被稱為中國的「南大門」。由於經濟水平發達、發展程度優秀，廣州與北京、上海、深圳並稱為中國內地四大一線城市。據聯合國《2016年中國城市可持續發展報告：衡量生態投入與人類發展》顯示，廣州人類發展指數蟬聯中國第一。',\n",
       "       '4fc32aa8-5d9c-4427-a379-94a825f97c5d', '有別稱仙城的城市的歷史大約有多少年甚至更長？',\n",
       "       '139', '2200'],\n",
       "      dtype='<U1158')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build(vocab,weight_matrix,max_review_length,question_length):\n",
    "    hidden_layer_p = 128\n",
    "    hidden_layer_q = 128\n",
    "    \n",
    "    input_p = Input((max_review_length,),name='paragraph')\n",
    "    input_q = Input((question_length,),name='question')\n",
    "\n",
    "    # input_dim is len(vocab) like the account of word. \n",
    "    embd_p = Embedding(input_dim=len(vocab),\n",
    "                     output_dim=weight_matrix.shape[1],\n",
    "                     input_length=max_review_length,\n",
    "                     weights=[weight_matrix])(input_p)\n",
    "    embd_q = Embedding(input_dim=len(vocab),\n",
    "                     output_dim=weight_matrix.shape[1],\n",
    "                     input_length=question_length,\n",
    "                     weights=[weight_matrix])(input_q)\n",
    "\n",
    "    encoder_p = LSTM(hidden_layer_p,return_sequences=True,name='paragraph_encoder')\n",
    "    enco_p = encoder_p(embd_p)\n",
    "    \n",
    "    encoder_q = LSTM(hidden_layer_q,name='question_encoder')\n",
    "    enco_q = encoder_q(embd_q)\n",
    "    #consistent dimension as paragraph\n",
    "    enco_q = RepeatVector(max_review_length)(enco_q)\n",
    "\n",
    "    # use encoded question to attent paragraph\n",
    "    attn = concatenate([enco_p,enco_q],name='QP_concat')\n",
    "    attn = TimeDistributed(Dense(5,activation='tanh'))(attn)\n",
    "    attn = TimeDistributed(Dense(1,activation='tanh'))(attn)\n",
    "    attn = Lambda(lambda x: softmax(x,axis=-2),name='attention_by_softmax')(attn)\n",
    "\n",
    "    ## encode for comprehensing\n",
    "    comprehension_p = LSTM(hidden_layer_p,go_backwards=True,return_sequences=True,name='compre_encoder')(embd_p)\n",
    "\n",
    "    # find start of answer\n",
    "    first_comprehension_p = Multiply(name='first_attn_on_compre_vector')([attn,comprehension_p])\n",
    "    first_deco_p = LSTM(hidden_layer_p,return_sequences=True,name='first_ecoder_p')(first_comprehension_p)\n",
    "    first_ptr_attn = TimeDistributed(Dense(1,activation='tanh'))(first_deco_p)\n",
    "    first_ptr_attn_value = Lambda(lambda x: softmax(x,axis=-2),name='first_point_layer')(first_ptr_attn)\n",
    "    \n",
    "    # find end of answer\n",
    "    second_comprehension_p = Multiply(name='second_attn_on_compre_vector')([first_ptr_attn_value,comprehension_p])\n",
    "    second_deco_p = LSTM(hidden_layer_p,return_sequences=True,name='second_decoder_p')(second_comprehension_p)\n",
    "    second_ptr_attn = TimeDistributed(Dense(1,activation='tanh'))(second_deco_p)\n",
    "    second_ptr_attn_value = Lambda(lambda x: softmax(x,axis=-2),name='second_point_layer')(second_ptr_attn)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[input_p,input_q],outputs=[first_ptr_attn_value,second_ptr_attn_value])\n",
    "    model.compile(loss='mse',optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "question (InputLayer)            (None, 332)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "paragraph (InputLayer)           (None, 1159)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 332, 256)      1095936     question[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1159, 256)     1095936     paragraph[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "question_encoder (LSTM)          (None, 128)           197120      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "paragraph_encoder (LSTM)         (None, 1159, 128)     197120      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)   (None, 1159, 128)     0           question_encoder[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "QP_concat (Concatenate)          (None, 1159, 256)     0           paragraph_encoder[0][0]          \n",
      "                                                                   repeat_vector_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistribu (None, 1159, 5)       1285        QP_concat[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistribu (None, 1159, 1)       6           time_distributed_5[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "attention_by_softmax (Lambda)    (None, 1159, 1)       0           time_distributed_6[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "compre_encoder (LSTM)            (None, 1159, 128)     197120      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "first_attn_on_compre_vector (Mul (None, 1159, 128)     0           attention_by_softmax[0][0]       \n",
      "                                                                   compre_encoder[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "first_ecoder_p (LSTM)            (None, 1159, 128)     131584      first_attn_on_compre_vector[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistribu (None, 1159, 1)       129         first_ecoder_p[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "first_point_layer (Lambda)       (None, 1159, 1)       0           time_distributed_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "second_attn_on_compre_vector (Mu (None, 1159, 128)     0           first_point_layer[0][0]          \n",
      "                                                                   compre_encoder[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "second_decoder_p (LSTM)          (None, 1159, 128)     131584      second_attn_on_compre_vector[0][0\n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistribu (None, 1159, 1)       129         second_decoder_p[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "second_point_layer (Lambda)      (None, 1159, 1)       0           time_distributed_8[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 3,047,949\n",
      "Trainable params: 3,047,949\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_batch = 128\n",
    "n_epoch = 10\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = build(vocab , weight_matrix , max_review_length , question_length)\n",
    "#cosine_proximity\n",
    "model.summary()\n",
    "#hist = model.fit([p_train,q_train],[ans_train_start,ans_train_end],batch_size=n_batch,epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model , to_file='QASystem.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ans = model.predict([p_train[2:3],q_train[2:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['广州', '1',\n",
       "       '廣州市，簡稱穗，現有別稱五羊仙城、羊城、穗城、穗垣、仙城、花城，為中華人民共和國廣東省省會，中國超大城市及副省級城市，是繼上海、北京之後的中國第三大城市、國家中心城市、也是中國華南地區的經濟、文化、科技和教育中心及交通樞紐，是中國人民解放軍南部戰區聯合作戰指揮部所在地。廣州擁有2200年以上歷史，為中國首批歷史文化名城之一，是中國南方最大、歷史最悠久的對外通商口岸，也是全國首批對外開放的14個沿海城市之一，世界著名的港口城市之一。廣州港是中國第五大港口，世界第八大港口。廣州在2010年成功舉辦第16屆亞洲運動會。\\n廣州地處華南，廣東省的東南部，珠江三角洲中北緣，西江、北江、東江三江匯合處，瀕臨南中國海，東連東莞市和惠州市博羅、龍門兩縣，西鄰佛山市的三水、南海和順德三區，北靠清遠市的市區和佛岡縣及韶關市的新豐縣，南接中山市，毗鄰香港、澳門特別行政區，地理位置優越，是「海上絲綢之路」的起點之一，被稱為中國的「南大門」。由於經濟水平發達、發展程度優秀，廣州與北京、上海、深圳並稱為中國內地四大一線城市。據聯合國《2016年中國城市可持續發展報告：衡量生態投入與人類發展》顯示，廣州人類發展指數蟬聯中國第一。',\n",
       "       '76185dc3-7ee3-4e0c-9b46-4614c50d9354',\n",
       "       '2016年的人類發展指數第一的中國城市位在廣東省的哪一方？', '271', '東南'],\n",
       "      dtype='<U1158')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.squeeze(my_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0008630855, 127)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[index.argsort()[-1]],index.argsort()[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008630855"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
