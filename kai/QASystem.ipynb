{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "    P -> Encoder-lstm -> match-lstm -> PTR-Net\n",
    "    Q -> Encoder-lstm -^ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from numpy import array\n",
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,Input ,Lambda \n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM , Embedding\n",
    "from keras.layers import merge,Dot,Multiply,Reshape,concatenate,RepeatVector\n",
    "\n",
    "#from keras.backend import softmax #無法設定axis\n",
    "from keras.activations import softmax #可設定axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_p = 128\n",
    "hidden_layer_q = 128\n",
    "\n",
    "\n",
    "\n",
    "max_review_length = 5000\n",
    "question_length = 20\n",
    "\n",
    "input_p = Input((max_review_length,),name='paragraph')\n",
    "input_q = Input((question_length,),name='question')\n",
    "\n",
    "# input_dim is len(vocab) like the account of word. \n",
    "embd = Embedding(input_dim=100,output_dim=128)\n",
    "\n",
    "embd_p = embd(input_p)\n",
    "embd_q = embd(input_q)\n",
    "\n",
    "encoder_p = LSTM(hidden_layer_p,return_sequences=True,name='paragraph_encoder')\n",
    "encoder_q = LSTM(hidden_layer_q,name='question_encoder')\n",
    "enco_p = encoder_p(embd_p)\n",
    "enco_q = encoder_q(embd_q)\n",
    "\n",
    "enco_q = RepeatVector(max_review_length)(enco_q)\n",
    "\n",
    "attn = concatenate([enco_p,enco_q],name='QP_concat')\n",
    "\n",
    "attn = TimeDistributed(Dense(5,activation='tanh'))(attn)\n",
    "attn = TimeDistributed(Dense(1,activation='tanh'))(attn)\n",
    "attn = Lambda(lambda x: softmax(x,axis=-2),name='attention_by_softmax')(attn)\n",
    "\n",
    "#attn_vector = Multiply()([attn,enco_p])\n",
    "\n",
    "## decoder\n",
    "comprehension_p = LSTM(hidden_layer_p,go_backwards=True,return_sequences=True,name='compre_encoder')(embd_p)\n",
    "\n",
    "attn_comprehension_p = Multiply(name='attn_on_compre_vector')([attn,comprehension_p])\n",
    "\n",
    "deco_p = LSTM(hidden_layer_p,return_sequences=True,name='decoder_p')(attn_comprehension_p)\n",
    "\n",
    "ptr_attn = TimeDistributed(Dense(1,activation='tanh'))(deco_p)\n",
    "ptr_attn_value = Lambda(lambda x: softmax(x,axis=-2),name='point_layer')(ptr_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "paragraph (InputLayer)           (None, 5000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "question (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_27 (Embedding)         multiple              12800       paragraph[0][0]                  \n",
      "                                                                   question[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "question_encoder (LSTM)          (None, 128)           131584      embedding_27[1][0]               \n",
      "____________________________________________________________________________________________________\n",
      "paragraph_encoder (LSTM)         (None, 5000, 128)     131584      embedding_27[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_16 (RepeatVector)  (None, 5000, 128)     0           question_encoder[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "QP_concat (Concatenate)          (None, 5000, 256)     0           paragraph_encoder[0][0]          \n",
      "                                                                   repeat_vector_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_35 (TimeDistrib (None, 5000, 5)       1285        QP_concat[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistrib (None, 5000, 1)       6           time_distributed_35[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "attention_by_softmax (Lambda)    (None, 5000, 1)       0           time_distributed_36[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "compre_encoder (LSTM)            (None, 5000, 128)     131584      embedding_27[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "attn_on_compre_vector (Multiply) (None, 5000, 128)     0           attention_by_softmax[0][0]       \n",
      "                                                                   compre_encoder[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "decoder_p (LSTM)                 (None, 5000, 128)     131584      attn_on_compre_vector[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistrib (None, 5000, 1)       129         decoder_p[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "point_layer (Lambda)             (None, 5000, 1)       0           time_distributed_37[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 540,556\n",
      "Trainable params: 540,556\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input_p,input_q],outputs=[ptr_attn_value])\n",
    "model.compile(loss='cosine_proximity',optimizer='adam') #cosine_proximity\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model , to_file='QASystem.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
